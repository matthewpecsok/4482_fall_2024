{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZvGyUOUIT1CaH_Io0BDSGE5oBLe9z-Dm",
      "authorship_tag": "ABX9TyOvtV21FUprBppKZi28n+wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewpecsok/4482_fall_2022/blob/main/tutorials/4482_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gihdLJecXNkr"
      },
      "source": [
        "This notebook explores ensemble methods for model creation. We start with a very simple decision tree and then dive into various ensemble methods.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/ensemble.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW6HwHTYVBRM"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7aQHKluWre9"
      },
      "source": [
        "titanic= pd.read_csv('https://raw.githubusercontent.com/matthewpecsok/4482_fall_2022/main/data/titanic_cleaned.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZycUrcyRWxaA"
      },
      "source": [
        "titanic.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Tz86e2W8v1"
      },
      "source": [
        "titanic.Pclass = titanic.Pclass.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVgUr-pSXP-A"
      },
      "source": [
        "titanic.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6veLSX7XQ9t"
      },
      "source": [
        "titanic_enc = pd.get_dummies(titanic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP8GWOd4XWbH"
      },
      "source": [
        "y_target = titanic_enc.pop('Survived')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBy3eveMMmu6"
      },
      "source": [
        "Below we will explore a variety of different decision tree models. we begin with a simple default param model, then add in grid search, move to random forest, and finally a boosting classifier. It's worth noting that each of these methods have strengths and weaknesses. Ensemble methods tend to perform better than their more simple counterparts, but as you will see below it's not always quite that simple. Just throwing an ensemble method at a problem won't always improve on score, the algorithm must be implemented correctly to take advantage of the classifiers strengths and minimize the weaknesses. \n",
        "\n",
        "One easy to see tradeoff is complexity. The basic decision tree runs in a fraction of the time of these more complex methods. Especially when combined with gridsearch the ensembles are building hundreds to thousands more models than even a grid search decision tree. \n",
        "\n",
        "Our dataset is a little over 700 rows. Imagine how long these would take on a dataset with tens of thousands or millions of rows. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtpwB1dHH8KA"
      },
      "source": [
        "# Decision Tree with default params\n",
        "Not an ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvOQU09JH_AK"
      },
      "source": [
        "clf = DecisionTreeClassifier().fit(titanic_enc, y_target)\n",
        "pd.DataFrame(cross_validate(clf,titanic_enc,y_target,scoring=['f1'])).agg('mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQowcDe4F9K8"
      },
      "source": [
        "# Decision Tree Classifier with GridSearchCV\n",
        "still not ensemble, but clearly more complex than training a single tree. This explores the parameter space far more thoroughly to find the best performing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fvhbW8bYOW-"
      },
      "source": [
        "parameters = {'max_depth':list(range(1,20,2))}\n",
        "clf = GridSearchCV(DecisionTreeClassifier(), parameters,scoring=['f1'],refit=False)\n",
        "clf = clf.fit(titanic_enc, y_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ_yxrAplvL7"
      },
      "source": [
        "results = clf.cv_results_\n",
        "pd.DataFrame(results)[['param_max_depth','mean_test_f1']].sort_values('mean_test_f1',ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MptdDiLtGCh8"
      },
      "source": [
        "# Random Forest (Bagging) Classifier with GridSearchCV\n",
        "\n",
        "A bagging model in which predictions are averaged. Bagging often performs better with very deep trees which in general would overfit if it weren't for the averaging of predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eup5Kcera-da"
      },
      "source": [
        "parameters = {'max_depth':list(range(1,20,2)),\n",
        "              'n_estimators':[1,2,3,4,5,6,7,8,9,10,100,200,300,400,500,600,700,900,1000]}\n",
        "clf = GridSearchCV(RandomForestClassifier(n_jobs=-1), parameters,scoring=['f1'],refit=False)\n",
        "clf = clf.fit(titanic_enc, y_target)\n",
        "results = clf.cv_results_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL5YL8h5cf1L"
      },
      "source": [
        "pd.DataFrame(results)[['param_max_depth','param_n_estimators','mean_test_f1']].sort_values('mean_test_f1',ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-G6ZPGYBOms"
      },
      "source": [
        "list(range(1,20,2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJLg96TNGId2"
      },
      "source": [
        "# Gradient Boosting Classifier with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-y8Za7geZsX"
      },
      "source": [
        "\n",
        "\n",
        "parameters = {'max_depth':list(range(1,20,2)),\n",
        "              'n_estimators':[1,2,3,4,5,6,7,8,9,10,100,200,300,400,500,600]\n",
        "              }\n",
        "clf = GridSearchCV(GradientBoostingClassifier(), parameters,scoring=['f1'],refit=False)\n",
        "clf = clf.fit(titanic_enc, y_target)\n",
        "results = clf.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pURWmTfliqx7"
      },
      "source": [
        "pd.DataFrame(results)[['param_n_estimators','param_max_depth','mean_test_f1']].sort_values('mean_test_f1',ascending=False).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R92bvBDmDV2e"
      },
      "source": [
        "# Voting Ensemble\n",
        "\n",
        "Predicts the class that the majority of classifiers predicted. In this case if we have the three classifiers predict\n",
        "\n",
        "1 predicts class 0\n",
        "\n",
        "2 predicts class 1\n",
        "\n",
        "3 predicts class 0\n",
        "\n",
        "then the final prediction will be 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDEMxmJhlKO3"
      },
      "source": [
        "clf1 = SVC(kernel='sigmoid',random_state=1)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = SVC(kernel='rbf',random_state=1)\n",
        "\n",
        "eclf = VotingClassifier(\n",
        "    estimators=[('svc_sig', clf1), ('rf', clf2), ('svc_rbf', clf3)],\n",
        "    voting='hard')\n",
        "\n",
        "\n",
        "eclf = eclf.fit(titanic_enc, y_target)\n",
        "pd.DataFrame(cross_validate(eclf,titanic_enc,y_target,scoring=['f1'])).agg('mean')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrgPS89XQE9f"
      },
      "source": [
        "# Voting with Grid Search\n",
        "\n",
        "Here we show that it is possible to combine our use of GridSearch, Cross-Validation and Ensemble methods. While the performance is not as good in this case it is a very robust method that can outperform in some circumstances. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8DT7kuqFRau"
      },
      "source": [
        "# dropping the number of grid search params to reduce fit time\n",
        "\n",
        "# start with a sigmoid SVC\n",
        "parameters_svc_sig = {'C':list(range(1,10,2))}\n",
        "clf1 = GridSearchCV(SVC(kernel='sigmoid',random_state=1), parameters_svc_sig,scoring='f1',refit=True)\n",
        "\n",
        "\n",
        "# Add in a random forest\n",
        "parameters_dt = {'max_depth':list(range(1,20,2)),\n",
        "                'n_estimators':[1,2,3,4,5,6,7,8,9,10,100,200]\n",
        "                 }\n",
        "clf2 = GridSearchCV(RandomForestClassifier(), parameters_dt,scoring='f1',refit=True)\n",
        "\n",
        "# and finally a SVC with a radial kernel\n",
        "parameters_svc_rbf = {'C':list(range(1,10,2))}\n",
        "clf3 = GridSearchCV(SVC(kernel='rbf',random_state=1), parameters_svc_rbf,scoring='f1',refit=True)\n",
        "\n",
        "eclf = VotingClassifier(\n",
        "    estimators=[('svc_sig', clf1), ('rf', clf2), ('svc_rbf', clf3)],\n",
        "    voting='hard')\n",
        "\n",
        "\n",
        "eclf = eclf.fit(titanic_enc, y_target)\n",
        "pd.DataFrame(cross_validate(eclf,titanic_enc,y_target,scoring=['f1'])).agg('mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF4eHWLzlM3o"
      },
      "source": [
        "# Stacked Ensemble\n",
        "\n",
        "Stacking is one of the more complex models we will explore in this class. It combines multiple models each trained using a cross validated dataset into a new single gradient boosted model which is then trained using cross validation again. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VilGJBcEHnci"
      },
      "source": [
        "clf1 = SVC(kernel='sigmoid',random_state=1)\n",
        "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "clf3 = SVC(kernel='rbf',random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhjRzXWKkWLB"
      },
      "source": [
        "estimators = [('svc_sig', clf1),\n",
        "              ('rf', clf2),\n",
        "              ('svc_rbf', clf3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_6sLACJO0Tl"
      },
      "source": [
        "\n",
        "final_estimator = GradientBoostingClassifier(\n",
        "    n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\n",
        "    random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "stacked_ens = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator)\n",
        "\n",
        "pd.DataFrame(cross_validate(stacked_ens,titanic_enc,y_target,scoring=['f1'])).agg('mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyNp-TEDltbp"
      },
      "source": [
        "# Stacking Ensemble with GridSearchCV\n",
        "\n",
        "Now we stack but throw in grid search to try and get the best single model for each classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U_ByVCNlgwz"
      },
      "source": [
        "parameters_svc_sig = {'C':list(range(1,3,1))}\n",
        "clf1 = GridSearchCV(SVC(kernel='sigmoid',random_state=1), parameters_svc_sig,scoring='f1',refit=True)\n",
        "\n",
        "\n",
        "# Add in a random forest\n",
        "parameters_dt = {'max_depth':list(range(1,3,1)),\n",
        "                'n_estimators':[1,2,3,4,5] # reduce the grid search to make the model fit faster (it won't be as robust though)\n",
        "                 }\n",
        "clf2 = GridSearchCV(RandomForestClassifier(), parameters_dt,scoring='f1',refit=True)\n",
        "\n",
        "# and finally a SVC with a radial kernel\n",
        "parameters_svc_rbf = {'C':list(range(1,3,1))}\n",
        "clf3 = GridSearchCV(SVC(kernel='rbf',random_state=1), parameters_svc_rbf,scoring='f1',refit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWZ33seQly62"
      },
      "source": [
        "estimators = [('svc_sig', clf1),\n",
        "              ('rf', clf2),\n",
        "              ('svc_rbf', clf3)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKXwp71RoI1v"
      },
      "source": [
        "note that max_features = 1. this new feature is actually the prediction from each of the estimators "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUL3x2rmlWZ-"
      },
      "source": [
        "# note that the performance here will be underwhelming as combinging grid search with a stacking classifier is a computationally expensive \n",
        "# procedure. previous runs of this using the same grid params as models above took over 2 hours to run\n",
        "# this code is mainly for demonstration purposes therefore and should not be interpreted as a \n",
        "# reason to not consinder stacking due to poor performance\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "\n",
        "final_estimator = GradientBoostingClassifier(\n",
        "    n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\n",
        "    random_state=42)\n",
        "\n",
        "\n",
        "stacked_ens = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=final_estimator)\n",
        "\n",
        "pd.DataFrame(cross_validate(stacked_ens,titanic_enc,y_target,scoring=['f1'])).agg('mean')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYEIrEUl-tee"
      },
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/4482_ensemble.ipynb\" ./\n",
        "\n",
        "# run the second shell command, jupyter nbconvert --to html \"file name of the notebook\"\n",
        "# create html from ipynb\n",
        "\n",
        "!jupyter nbconvert --to html \"4482_ensemble.ipynb\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}