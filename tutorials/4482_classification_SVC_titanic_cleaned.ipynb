{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewpecsok/4482_fall_2024/blob/main/tutorials/4482_classification_SVC_titanic_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWVgdVutJP98"
      },
      "source": [
        "Welcome to the Support Vector classification notebook. Please start with the Hyperparameter Tuning notebook if you have not already. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKEFgC7Ycm_"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbDv78ea7uk-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix,\\\n",
        " recall_score, precision_score, f1_score, accuracy_score, make_scorer,\\\n",
        "  precision_recall_fscore_support\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqyRu9C4YfJy"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL9otp_39m7r"
      },
      "outputs": [],
      "source": [
        "titanic_cleaned = pd.read_csv('https://raw.githubusercontent.com/matthewpecsok/4482_fall_2024/main/data/titanic_cleaned.csv').drop('Cabin', axis=1) # drop cabin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifk5nluWDuf7"
      },
      "outputs": [],
      "source": [
        "titanic_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebBw5oSG4SJd"
      },
      "outputs": [],
      "source": [
        "titanic_cleaned['Pclass'] = titanic_cleaned.Pclass.astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZSu68HH4ceS"
      },
      "outputs": [],
      "source": [
        "titanic_cleaned.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPWZ7XI44Dqw"
      },
      "outputs": [],
      "source": [
        "y = titanic_cleaned.pop('Survived')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuEa_lDSW1-_"
      },
      "outputs": [],
      "source": [
        "X = pd.get_dummies(titanic_cleaned)\n",
        "print(X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQa9YZviloKQ"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9iyfH2CYgR8"
      },
      "source": [
        "## SVC Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsIzyTF9YhYj"
      },
      "source": [
        "### GriSearchCV \n",
        "\n",
        "Exploring multiple SVC models using grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ogJD_Mq3SSv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.arange(0,10000,200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVG64ZKC5Euh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'C':[5,10,15,20,30,40,50,60,70,80,90,100],\n",
        "              'kernel':['rbf','poly','sigmoid']\n",
        "              }\n",
        "svc = SVC(random_state=42)\n",
        "clf = GridSearchCV(svc, parameters,scoring='f1')\n",
        "clf.fit(X, y)\n",
        "\n",
        "clf.score(X, y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHsRQG0RX3iY"
      },
      "outputs": [],
      "source": [
        "clf.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBaNEttvNyam"
      },
      "outputs": [],
      "source": [
        "grid_search_df = pd.DataFrame(clf.cv_results_)\n",
        "print(grid_search_df.shape) \n",
        "grid_search_df.sort_values('mean_test_score',ascending=False)#.head() #only taking the top five rows as this is a large dataframe sort by the best f1 scores found"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrjN2SVjXeda"
      },
      "source": [
        "visualize the f1 score by hyperparameter C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hl_ld0tWeiq"
      },
      "outputs": [],
      "source": [
        "colors = {'sigmoid':'orange','rbf':'red', 'poly':'green' }\n",
        "\n",
        "\n",
        "sns.lmplot(x='param_C', y='mean_test_score', data=grid_search_df,palette=colors, hue='param_kernel', fit_reg=False)\n",
        "plt.title(\"all models mean F1 score optimized by C\")\n",
        "plt.show()\n",
        "\n",
        "# Notice how the poly kernel is substantially worse than the rbf kernel. \n",
        "# on the rb kernel we seem to maximize the f1 with a C of 60\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy1PiTaSZINa"
      },
      "outputs": [],
      "source": [
        "sns.lmplot(x='param_C', y='rank_test_score', data=grid_search_df,palette=colors, hue='param_kernel', fit_reg=False)\n",
        "plt.title(\"all models rank as optimized by C\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esBJ4s2WXkMi"
      },
      "source": [
        "visualize the grid search rank by C\n",
        "in this case lower is better (rank 1 is the best rank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xfr1x2Da8yS9"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(cross_validate(clf, \n",
        "               X,\n",
        "               y,\n",
        "               cv = 3,\n",
        "               return_train_score=True,\n",
        "               scoring=['accuracy','recall','precision','f1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyYW-h3_FQ9E"
      },
      "source": [
        "To conclude we evaluated 36 hyperparameter combinations and achieved substantially better performance on some models compared to others. our best performing model achieved an F1 score of 71.5% compared to an F1 of 29.6%. That's an increase of 41.9%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jRABcPlv0Os"
      },
      "source": [
        "What if you have potentially thousands of combinations? It may not be practical to run all the models. In that case randomizing your search and randomly picking combinations from the total number of possible models and selecting the best estimator is a good strategy. For example: in the decision tree model we have a parameter cc_alpha, which has an infinite number of possible values to choose from one cannot simply enter all the possible values so one must sample a subset of them from a uniform distribution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd0V-GlgwPIH"
      },
      "outputs": [],
      "source": [
        "parameters = {'C':[5,10,15,20,30,40,50,60,70,80,90,100],\n",
        "              'kernel':['rbf','poly','sigmoid']\n",
        "              }\n",
        "svc = SVC(random_state=42)\n",
        "clf = RandomizedSearchCV(svc, parameters,scoring='f1')\n",
        "clf.fit(X, y)\n",
        "\n",
        "clf.score(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg_AK59zxQ-B"
      },
      "outputs": [],
      "source": [
        "grid_search_df = pd.DataFrame(clf.cv_results_)\n",
        "print(grid_search_df.shape) \n",
        "grid_search_df.sort_values('mean_test_score',ascending=False)#.head() #only taking the top five rows as this is a large dataframe sort by the best f1 scores found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdJ0JoKTwkQl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
